{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import random as rd\n",
    "import argparse\n",
    "import os, sys\n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "class County:\n",
    "    def __init__(self,parsename):\n",
    "        self.parsename = parsename\n",
    "        dataframe = xls.parse(parsename)\n",
    "        self.data = dataframe\n",
    "    def disp_all(self):\n",
    "        print self.dataframe\n",
    "    def get_all(self):\n",
    "        return self.dataframe\n",
    "#xls = pd.ExcelFile('ME-mean-dry-dew.xlsx',header = None)\n",
    "#xls = pd.ExcelFile('ME-NOTEMP-JAN.xlsx',header = None)\n",
    "xls = pd.ExcelFile('ME-TEST-READY.xlsx',header = None)\n",
    "Zonal = []\n",
    "Zonal.append(County('ME'))\n",
    "ZonalNum = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# histo load based model stage 2\n",
    "#num_epoches = 10000# training epoches for each customer samples\n",
    "n_input_h = 63*24 # input size\n",
    "input_seq_size_h = n_input_h\n",
    "test_batch_size_h = 31 # days of a batch\n",
    "valid_batch_size_h = 0\n",
    "train_batch_size_h = 31\n",
    "data_dim_h = 1 # same time of a week\n",
    "n_output_h = 24\n",
    "output_seq_size_h = 24\n",
    "gap_h = 91\n",
    "n_hidden_h_1= 30\n",
    "n_hidden_h_2 =30\n",
    "n_hidden_h_3 =30\n",
    "tao_h = 0.5\n",
    "\n",
    "# feature based model stage 1\n",
    "#num_epoches = 30000 # training epoches for each customer samples\n",
    "out_thresh = 18499\n",
    "day_steps_f = 24\n",
    "val_rate_f = 0.0\n",
    "test_batch_size_f = test_batch_size_h*day_steps_f # days of a batch\n",
    "valid_batch_size_f = valid_batch_size_h*day_steps_f\n",
    "train_batch_size_f = train_batch_size_h*day_steps_f\n",
    "n_output_f = 1\n",
    "n_hidden_f_1 = 20\n",
    "n_hidden_f_2 = 20\n",
    "n_hidden_f_3 = 20\n",
    "n_hidden_f_4 = 20\n",
    "tao_f = 0.1\n",
    "gap_test_f = 10\n",
    "batch_size_f = test_batch_size_f # in this version, batch_size set same\n",
    "preserve_f = 16115 ## amount of first time points without complete features\n",
    "dropout_f = 0.9\n",
    "data_norm = 0.0\n",
    "# merge part stage 3\n",
    "s3_input = day_steps_f\n",
    "n_hidden_s3_1 = 60\n",
    "n_hidden_s3_2 = 60\n",
    "s3_output = 24\n",
    "n_epochs = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DEMAND MATRIX 9 X LENGTH, 9: INC is total, index with 0, other substations are from 1 -> 8\n",
    "rows_f = np.array(Zonal[0].data).shape[0]-0\n",
    "columns_f = np.array(Zonal[0].data).shape[1]\n",
    "database_f = np.zeros((ZonalNum, rows_f, columns_f))\n",
    "for i in range(ZonalNum):\n",
    "    tmp = np.array(Zonal[i].data)\n",
    "    database_f[i] = tmp.astype(np.float32)[:,:]\n",
    "database_f = np.transpose(database_f, [1, 0, 2])\n",
    "totalen_f = rows_f\n",
    "n_input_f = columns_f - 1\n",
    "data_norm = np.max(database_f[:,:,0])\n",
    "database_f[:,:,0] = database_f[:,:,0]/data_norm\n",
    "db_f = database_f\n",
    "\n",
    "rows = np.array(Zonal[0].data).shape[0]-0\n",
    "columns = np.array(Zonal[0].data).shape[1]\n",
    "# DEMAND MATRIX 9 X LENGTH, 9: INC is total, index with 0, other substations are from 1 -> 8\n",
    "database_h = np.zeros((rows/output_seq_size_h, output_seq_size_h, columns))\n",
    "tmp = np.array(Zonal[0].data)\n",
    "tmp = tmp.astype(np.float32)[:,:]\n",
    "database_h = tmp.reshape([rows/output_seq_size_h, output_seq_size_h, columns])\n",
    "totalen_h = rows/output_seq_size_h\n",
    "database_h[:,:,0] = database_h[:,:,0]/data_norm\n",
    "db_h = database_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define id arrays\n",
    "test_id_f = np.array(test_batch_size_f)\n",
    "#valid_id_f = np.array(valid_batch_size_f)\n",
    "#train_id_f = np.array(totalen_f - test_batch_size_f - valid_batch_size_f)\n",
    "\n",
    "#give values to id arrays\n",
    "#rang = range(preserve_f, totalen_f - test_batch_size_f)\n",
    "#valid_id_f = rd.sample(rang,valid_batch_size_f)\n",
    "test_id_f = np.array(range(totalen_f - test_batch_size_f,totalen_f))\n",
    "#train_id_f = set(range(preserve_f, totalen_f - test_batch_size_f)) - set(valid_id_f)\n",
    "\n",
    "#sort three id array\n",
    "#valid_id_f = np.sort(valid_id_f)\n",
    "test_id_f = np.sort(test_id_f)\n",
    "#train_id_f = np.array(list(train_id_f))\n",
    "\n",
    "#define id arrays\n",
    "test_id_h = np.array(test_batch_size_h)\n",
    "valid_id_h = np.array(valid_batch_size_h)\n",
    "train_id_h = np.array(totalen_h-test_batch_size_h-valid_batch_size_h-input_seq_size_h)\n",
    "\n",
    "id_start = preserve_f/24 + 1\n",
    "#give values to id arrays\n",
    "rang_h = range(input_seq_size_h/output_seq_size_h + id_start + gap_h,totalen_h-test_batch_size_h)\n",
    "valid_id_h = rd.sample(rang_h,valid_batch_size_h)\n",
    "test_id_h = np.array(range(totalen_h-test_batch_size_h,totalen_h))\n",
    "train_id_h = set(range(input_seq_size_h/output_seq_size_h + gap_h + id_start,totalen_h-test_batch_size_h))-set(valid_id_h)\n",
    "\n",
    "#sort three id array\n",
    "valid_id_h = np.sort(valid_id_h)\n",
    "test_id_h = np.sort(test_id_h)\n",
    "train_id_h = np.array(list(train_id_h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_X_f = tf.placeholder(tf.float32, [batch_size_f, ZonalNum, n_input_f], name = \"X_f\")\n",
    "_Y_f = tf.placeholder(tf.float32, [batch_size_f, ZonalNum, n_output_f], name = \"Y_f\")\n",
    "_Dropout_f = tf.placeholder(tf.float32, name = \"Dropout_f\")\n",
    "_X_h = tf.placeholder(tf.float32, [test_batch_size_h, n_input_h])\n",
    "_Y_h = tf.placeholder(tf.float32, [test_batch_size_h, n_output_h])\n",
    "\n",
    "# Create model\n",
    "def MLP_f(x, _dropout, weights, biases):    \n",
    "    x = tf.reshape(x, [-1, n_input_f])\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.sigmoid(layer_1)\n",
    "    #layer_1 = tf.nn.dropout(layer_1,_dropout)\n",
    "    \n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.sigmoid(layer_2)\n",
    "    #layer_2 = tf.nn.dropout(layer_2,_dropout)\n",
    "    \n",
    "    # Hidden layer with RELU activation\n",
    "    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
    "    layer_3 = tf.nn.sigmoid(layer_3)\n",
    "    #layer_3 = tf.nn.dropout(layer_3,_dropout)\n",
    "    \n",
    "    # Hidden layer with RELU activation\n",
    "    layer_4 = tf.add(tf.matmul(layer_3, weights['h4']), biases['b4'])\n",
    "    layer_4 = tf.nn.sigmoid(layer_4)\n",
    "\n",
    "    # Output layer with linear activation\n",
    "    result = tf.matmul(layer_4, weights['out']) + biases['out']\n",
    "    result = tf.nn.sigmoid(result)\n",
    "    #result = tf.nn.dropout(result,_dropout)\n",
    "    return result\n",
    "\n",
    "\n",
    "def MLP_h(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.sigmoid(layer_1)\n",
    "    #layer_1 = tf.nn.dropout(layer_1,_dropout)\n",
    "\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.sigmoid(layer_2)\n",
    "    #layer_2 = tf.nn.dropout(layer_2,_dropout)\n",
    "\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
    "    layer_3 = tf.nn.sigmoid(layer_3)\n",
    "    #layer_3 = tf.nn.dropout(layer_3,_dropout)\n",
    "\n",
    "    # Output layer with linear activation\n",
    "    result = tf.matmul(layer_3, weights['out']) + biases['out']\n",
    "    result = tf.nn.sigmoid(result)\n",
    "    #result = tf.nn.dropout(result,_dropout)\n",
    "    return result\n",
    "\n",
    "# MLP\n",
    "weights_f = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input_f, n_hidden_f_1]), name = \"w_f_1\"),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_f_1, n_hidden_f_2]), name = \"w_f_2\"),\n",
    "    'h3': tf.Variable(tf.random_normal([n_hidden_f_2, n_hidden_f_3]), name = \"w_f_3\"),\n",
    "    'h4': tf.Variable(tf.random_normal([n_hidden_f_3, n_hidden_f_4]), name = \"w_f_4\"),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_f_4, n_output_f]), name = \"w_o\")\n",
    "}\n",
    "biases_f = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_f_1]), name = \"b_f_1\"),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_f_2]), name = \"b_f_2\"),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden_f_3]), name = \"b_f_3\"),\n",
    "    'b4': tf.Variable(tf.random_normal([n_hidden_f_4]), name = \"b_f_4\"),\n",
    "    'out': tf.Variable(tf.random_normal([n_output_f]), name = \"b_o\")\n",
    "}\n",
    "weights_h = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input_h, n_hidden_h_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_h_1, n_hidden_h_2])),\n",
    "    'h3': tf.Variable(tf.random_normal([n_hidden_h_2, n_hidden_h_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_h_3, n_output_h]))\n",
    "}\n",
    "biases_h = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_h_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_h_2])),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden_h_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_output_h]))\n",
    "}\n",
    "p_f = MLP_f(_X_f, _Dropout_f, weights_f, biases_f)\n",
    "p_h = MLP_h(_X_h,  weights_h, biases_h)\n",
    "p_f = tf.reshape(p_f, [test_batch_size_h, s3_input])\n",
    "p_h = tf.reshape(p_h, [test_batch_size_h, s3_input])\n",
    "tao_s3 = tf.placeholder(tf.float32)\n",
    "_I = tf.concat(1, [p_f,p_h])\n",
    "_O = _Y_h\n",
    "weights_s3 = {\n",
    "    'h1': tf.Variable(tf.random_normal([2*s3_input, n_hidden_s3_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_s3_1, n_hidden_s3_2])),\n",
    "    #'h3': tf.Variable(tf.random_normal([n_hidden_f_2, n_hidden_f_3])),\n",
    "    #'h4': tf.Variable(tf.random_normal([n_hidden_f_3, n_hidden_f_4])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_s3_2, s3_output]))\n",
    "}\n",
    "biases_s3 = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_s3_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_s3_2])),\n",
    "    #'b3': tf.Variable(tf.random_normal([n_hidden_f_3])),\n",
    "    #'b4': tf.Variable(tf.random_normal([n_hidden_f_4])),\n",
    "    'out': tf.Variable(tf.random_normal([s3_output]))\n",
    "}\n",
    "def MLP_s3(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.sigmoid(layer_1)\n",
    "    #layer_1 = tf.nn.dropout(layer_1,_dropout)\n",
    "\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.sigmoid(layer_2)\n",
    "    #layer_2 = tf.nn.dropout(layer_2,_dropout)\n",
    "    # Output layer with linear activation\n",
    "    result = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    result = tf.nn.sigmoid(result)\n",
    "    #result = tf.nn.dropout(result,_dropout)\n",
    "    return result\n",
    "\n",
    "_P = MLP_s3(_I, weights_s3, biases_s3)\n",
    "'''\n",
    "reshaped_outputs = tf.reshape(_P, [-1]) \n",
    "\n",
    "reshaped_results = tf.reshape(_O,[-1])\n",
    "\n",
    "    #cost function\n",
    "cost_s3 = 0.0\n",
    "for i in range(s3_input*test_batch_size_h):\n",
    "    tmp = tf.cond(reshaped_results[i]-reshaped_outputs[i]>=0, lambda:(reshaped_results[i]-reshaped_outputs[i])*tao_s3,lambda:(reshaped_outputs[i]-reshaped_results[i])*(1.0-tao_s3))\n",
    "    cost_s3 = tf.add(tmp,cost_s3)\n",
    "ccost_s3 = tf.reduce_mean(tf.pow(_O - _P,2)) \n",
    "\n",
    "optimizer_s3 = tf.train.AdamOptimizer(learning_rate=0.001, beta1 = 0.8, beta2 = 0.7).minimize(ccost_s3)\n",
    "optimizer_s4 = tf.train.AdamOptimizer(learning_rate=0.0002, beta1 = 0.8, beta2 = 0.7).minimize(cost_s3)\n",
    "'''\n",
    "#n_hidden_s3_3 = 30\n",
    "#n_hidden_s3_4 = 30\n",
    "\n",
    "\n",
    "def maxe(predictions, targets):\n",
    "    return np.max(abs(predictions-targets))\n",
    "\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "\n",
    "def mape(predictions, targets):\n",
    "    return np.mean(abs(predictions-targets)/targets)\n",
    "def variable_summaries(var, name):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor.\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.scalar_summary('mean/' + name, mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))\n",
    "        tf.scalar_summary('sttdev/' + name, stddev)\n",
    "        tf.scalar_summary('max/' + name, tf.reduce_max(var))\n",
    "        tf.scalar_summary('min/' + name, tf.reduce_min(var))\n",
    "        tf.histogram_summary(name, var)\n",
    "\n",
    "def test_data_gen():\n",
    "    X_h = np.zeros((test_batch_size_h, n_input_h))\n",
    "    Y_h = np.zeros((test_batch_size_h, n_output_h))\n",
    "    X_f = np.zeros((test_batch_size_f,ZonalNum,n_input_f))\n",
    "    Y_f = np.zeros((test_batch_size_f,ZonalNum,n_output_f))\n",
    "    count_h = 0\n",
    "    count_f = 0\n",
    "    for i in test_id_f:\n",
    "        Y_f[count_f] = db_f[i,:,:1]\n",
    "        X_f[count_f] = db_f[i,:,1:]\n",
    "        count_f = count_f + 1\n",
    "    for i in test_id_h:\n",
    "        Y_h[count_h,:] = (db_h[i,:,0])\n",
    "        X_h[count_h,:] = (db_h[i-input_seq_size_h/output_seq_size_h-gap_h:i-gap_h,:,0]).reshape(n_input_h)\n",
    "        count_h = count_h + 1\n",
    "    X_h = X_h.astype(np.float32)\n",
    "    Y_h = Y_h.astype(np.float32)\n",
    "    X_f = X_f.astype(np.float32)\n",
    "    Y_f = Y_f.astype(np.float32)\n",
    "    return (X_h,Y_h,X_f,Y_f)\n",
    "\n",
    "def train_data_gen():\n",
    "    X_h = np.zeros((train_batch_size_h, n_input_h))\n",
    "    Y_h = np.zeros((train_batch_size_h, n_output_h))\n",
    "    X_f = np.zeros((train_batch_size_f,ZonalNum,n_input_f))\n",
    "    Y_f = np.zeros((train_batch_size_f,ZonalNum,n_output_f))\n",
    "    count_h = 0\n",
    "    count_f = 0\n",
    "    rang = range(0,train_id_h.shape[0])\n",
    "    train_rd = rd.sample(rang,train_batch_size_h)\n",
    "    train_rd = np.sort(train_rd)\n",
    "    for i in train_rd:\n",
    "        k = train_id_h[i]\n",
    "        Y_h[count_h] = db_h[k,:,0]\n",
    "        X_h[count_h] = (db_h[k-input_seq_size_h/output_seq_size_h-gap_h:k-gap_h,:,0]).reshape(n_input_h)\n",
    "        for j in range(24):\n",
    "            Y_f[count_f] = db_f[k*24+j,:,:1]\n",
    "            X_f[count_f] = db_f[k*24+j,:,1:]\n",
    "            count_f = count_f + 1\n",
    "        count_h = count_h + 1\n",
    "    X_h = X_h.astype(np.float32)\n",
    "    Y_h = Y_h.astype(np.float32)\n",
    "    X_f = X_f.astype(np.float32)\n",
    "    Y_f = Y_f.astype(np.float32)\n",
    "    return (X_h,Y_h,X_f,Y_f)\n",
    "\n",
    "def valid_data_gen():\n",
    "    X_h = np.zeros((valid_batch_size_h, n_input_h))\n",
    "    Y_h = np.zeros((valid_batch_size_h, n_output_h))\n",
    "    X_f = np.zeros((valid_batch_size_f,ZonalNum,n_input_f))\n",
    "    Y_f = np.zeros((valid_batch_size_f,ZonalNum,n_output_f))\n",
    "    count_h = 0\n",
    "    count_f = 0\n",
    "    rang = range(0,valid_id_h.shape[0])\n",
    "    valid_rd = rd.sample(rang,train_batch_size_h)\n",
    "    valid_rd = np.sort(valid_rd)\n",
    "    for i in train_rd:\n",
    "        k = valid_id_h[i]\n",
    "        Y_h[count_h] = db_h[k,:,0]\n",
    "        X_h[count_h] = (db_h[k-input_seq_size_h/output_seq_size_h-gap_h:k-gap_h,:,0]).reshape(n_input_h)\n",
    "        for j in range(24):\n",
    "            Y_f[count_f] = db_f[k*24+j,:,:1]\n",
    "            X_f[count_f] = db_f[k*24+j,:,1:]\n",
    "            count_f = count_f + 1\n",
    "        count_h = count_h + 1\n",
    "    X_h = X_h.astype(np.float32)\n",
    "    Y_h = Y_h.astype(np.float32)\n",
    "    X_f = X_f.astype(np.float32)\n",
    "    Y_f = Y_f.astype(np.float32)\n",
    "    return (X_h,Y_h,X_f,Y_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TempDP = pd.read_csv('VT dew point shift.csv', header = None)\n",
    "TempDB = pd.read_csv('VT drybulb shift.csv', header = None)\n",
    "TDP = np.array(TempDP)[1:, 4:]\n",
    "TDB = np.array(TempDB)[1:, 4:]\n",
    "TDP = TDP.astype(np.float32)\n",
    "TDB = TDB.astype(np.float32)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "Plist = []\n",
    "saver = tf.train.Saver({\"wf1\":weights_f['h1'],\"wf2\":weights_f['h2'],\"wf3\":weights_f['h3'],\"wf4\":weights_f['h4'],\"wfo\":weights_f['out'],\"bf1\":biases_f['b1'],\"bf2\":biases_f['b2'],\"bf3\":biases_f['b3'],\"bf4\":biases_f['b4'],\"bfo\":biases_f['out'],'wh1':weights_h['h1'],'wh2':weights_h['h2'],'wh3':weights_h['h3'],'who':weights_h['out'],'bh1':biases_h['b1'],'bh2':biases_h['b2'],'bh3':biases_h['b3'],'bho':biases_h['out'],\"wf1\":weights_f['h1'],\"wf2\":weights_f['h2'],\"wf3\":weights_f['h3'],\"wf4\":weights_f['h4'],\"wfo\":weights_f['out'],\"bf1\":biases_f['b1'],\"bf2\":biases_f['b2'],\"bf3\":biases_f['b3'],\"bf4\":biases_f['b4'],\"bfo\":biases_f['out'],'wh1':weights_h['h1'],'wh2':weights_h['h2'],'wh3':weights_h['h3'],'who':weights_h['out'],'bh1':biases_h['b1'],'bh2':biases_h['b2'],'bh3':biases_h['b3'],'bho':biases_h['out'],\"ws1\":weights_s3['h1'],\"ws2\":weights_s3['h2'],\"wso\":weights_s3['out'],'bs1':biases_s3['b1'],'bs2':biases_s3['b2'],'bso':biases_s3['out']})\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "taolist = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "# get rmse prediction\n",
    "#sess.run(init)\n",
    "#saver.restore(sess, \"C-models/model-ME-rmse-hs.ckpt\")\n",
    "#x_h,y_h,x_f,y_f = test_data_gen()\n",
    "#P = sess.run(_P, feed_dict = {_X_f: x_f, _Y_f: y_f, _X_h: x_h, _Y_h: y_h, tao_s3: 0.1})\n",
    "#print \"Tao: \" + str(tmp_tao) + \" **** Pinball = \" + str(err_t) + \" **** RMSE = \" + str(np.sqrt(err_c))\n",
    "#P = np.reshape(np.array(P),[-1])\n",
    "#Prmse = P\n",
    "#Preal = y_f\n",
    "# get pinball predictions\n",
    "for t in xrange(0,9):\n",
    "    tmp_tao = taolist[t]\n",
    "    sess.run(init)\n",
    "    #saver.restore(sess, \"C-models/model-ME-rmse-pinball-\"+str(tmp_tao)+\"-hs.ckpt\")\n",
    "    saver.restore(sess,\"C-models/model-ME-rmse-pinball-\"+str(tmp_tao)+ \"-checkpoint-4c-hs.ckpt\")\n",
    "    x_h,y_h,x_f,y_f = test_data_gen()\n",
    "    P = sess.run(_P, feed_dict = {_X_f: x_f, _Y_f: y_f, _X_h: x_h, _Y_h: y_h, tao_s3: tmp_tao})\n",
    "    #print \"Tao: \" + str(tmp_tao) + \" **** Pinball = \" + str(err_t) + \" **** RMSE = \" + str(np.sqrt(err_c))\n",
    "    P = np.reshape(np.array(P),[-1])\n",
    "    Plist.append(P)\n",
    "\n",
    "# sort quantiles\n",
    "Plist = np.array(Plist)\n",
    "Plist = Plist*data_norm\n",
    "Plist = np.sort(Plist, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DataFrame(Plist.T).to_csv('prediction-CT.csv', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
