{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import random as rd\n",
    "import argparse\n",
    "import os, sys\n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as pl\n",
    "class County:\n",
    "    def __init__(self,parsename):\n",
    "        self.parsename = parsename\n",
    "        dataframe = xls.parse(parsename)\n",
    "        self.data = dataframe\n",
    "    def disp_all(self):\n",
    "        print self.dataframe\n",
    "    def get_all(self):\n",
    "        return self.dataframe\n",
    "xls = pd.ExcelFile('No-drybulb-dewpoint-short-dataset-CT.xlsx',header = None)\n",
    "Zonal = []\n",
    "Zonal.append(County('CT'))\n",
    "ZonalNum = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# histo load based model stage 2\n",
    "#num_epoches = 10000# training epoches for each customer samples\n",
    "n_input_h = 63*24 # input size\n",
    "input_seq_size_h = n_input_h\n",
    "test_batch_size_h = 28 # days of a batch\n",
    "valid_batch_size_h = 0\n",
    "train_batch_size_h = 28\n",
    "data_dim_h = 1 # same time of a week\n",
    "n_output_h = 24\n",
    "output_seq_size_h = 24\n",
    "gap_h = 91\n",
    "n_hidden_h_1= 30\n",
    "n_hidden_h_2 =30\n",
    "n_hidden_h_3 =30\n",
    "tao_h = 0.5\n",
    "bat = 91\n",
    "# feature based model stage 1\n",
    "#num_epoches = 30000 # training epoches for each customer samples\n",
    "out_thresh = 18499\n",
    "day_steps_f = 24\n",
    "val_rate_f = 0.0\n",
    "test_batch_size_f = test_batch_size_h*day_steps_f # days of a batch\n",
    "valid_batch_size_f = valid_batch_size_h*day_steps_f\n",
    "train_batch_size_f = train_batch_size_h*day_steps_f\n",
    "n_output_f = 1\n",
    "n_hidden_f_1 = 20\n",
    "n_hidden_f_2 = 20\n",
    "n_hidden_f_3 = 20\n",
    "n_hidden_f_4 = 20\n",
    "tao_f = 0.1\n",
    "gap_test_f = 10\n",
    "batch_size_f = test_batch_size_f # in this version, batch_size set same\n",
    "preserve_f = 16115 ## amount of first time points without complete features\n",
    "dropout_f = 0.9\n",
    "data_norm = 0.0\n",
    "# merge part stage 3\n",
    "s3_input = day_steps_f\n",
    "n_hidden_s3_1 = 60\n",
    "n_hidden_s3_2 = 60\n",
    "s3_output = 24\n",
    "n_epochs = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DEMAND MATRIX 9 X LENGTH, 9: INC is total, index with 0, other substations are from 1 -> 8\n",
    "rows_f = np.array(Zonal[0].data).shape[0]-0\n",
    "columns_f = np.array(Zonal[0].data).shape[1]\n",
    "database_f = np.zeros((ZonalNum, rows_f, columns_f))\n",
    "for i in range(ZonalNum):\n",
    "    tmp = np.array(Zonal[i].data)\n",
    "    database_f[i] = tmp.astype(np.float32)[:,:]\n",
    "database_f = np.transpose(database_f, [1, 0, 2])\n",
    "totalen_f = rows_f\n",
    "n_input_f = columns_f - 1\n",
    "data_norm = np.max(database_f[:,:,0])\n",
    "database_f[:,:,0] = database_f[:,:,0]/data_norm\n",
    "db_f = database_f\n",
    "\n",
    "rows = np.array(Zonal[0].data).shape[0]-0\n",
    "columns = np.array(Zonal[0].data).shape[1]\n",
    "# DEMAND MATRIX 9 X LENGTH, 9: INC is total, index with 0, other substations are from 1 -> 8\n",
    "database_h = np.zeros((rows/output_seq_size_h, output_seq_size_h, columns))\n",
    "tmp = np.array(Zonal[0].data)\n",
    "tmp = tmp.astype(np.float32)[:,:]\n",
    "database_h = tmp.reshape([rows/output_seq_size_h, output_seq_size_h, columns])\n",
    "totalen_h = rows/output_seq_size_h\n",
    "database_h[:,:,0] = database_h[:,:,0]/data_norm\n",
    "db_h = database_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3086 3087 3088 3089 3090 3091 3092 3093 3094 3095 3096 3097 3098 3099 3100\n",
      " 3101 3102 3103 3104 3105 3106 3107 3108 3109 3110 3111 3112 3113]\n",
      "[ 826  827  828 ..., 3020 3021 3022]\n"
     ]
    }
   ],
   "source": [
    "#define id arrays\n",
    "test_id_f = np.array(test_batch_size_f)\n",
    "#valid_id_f = np.array(valid_batch_size_f)\n",
    "#train_id_f = np.array(totalen_f - test_batch_size_f - valid_batch_size_f)\n",
    "\n",
    "#give values to id arrays\n",
    "#rang = range(preserve_f, totalen_f - test_batch_size_f)\n",
    "#valid_id_f = rd.sample(rang,valid_batch_size_f)\n",
    "test_id_f = np.array(range(totalen_f - test_batch_size_f,totalen_f))\n",
    "#train_id_f = set(range(preserve_f, totalen_f - test_batch_size_f)) - set(valid_id_f)\n",
    "\n",
    "#sort three id array\n",
    "#valid_id_f = np.sort(valid_id_f)\n",
    "test_id_f = np.sort(test_id_f)\n",
    "#train_id_f = np.array(list(train_id_f))\n",
    "\n",
    "#define id arrays\n",
    "test_id_h = np.array(test_batch_size_h)\n",
    "valid_id_h = np.array(valid_batch_size_h)\n",
    "train_id_h = np.array(totalen_h-test_batch_size_h-valid_batch_size_h-input_seq_size_h)\n",
    "\n",
    "id_start = preserve_f/24 + 1\n",
    "#give values to id arrays\n",
    "rang_h = range(input_seq_size_h/output_seq_size_h + id_start + gap_h,totalen_h-bat)\n",
    "valid_id_h = rd.sample(rang_h,valid_batch_size_h)\n",
    "test_id_h = np.array(range(totalen_h-test_batch_size_h,totalen_h))\n",
    "train_id_h = set(range(input_seq_size_h/output_seq_size_h + gap_h + id_start,totalen_h-bat))-set(valid_id_h)\n",
    "\n",
    "#sort three id array\n",
    "valid_id_h = np.sort(valid_id_h)\n",
    "test_id_h = np.sort(test_id_h)\n",
    "train_id_h = np.array(list(train_id_h))\n",
    "print test_id_h\n",
    "print train_id_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    _X_f = tf.placeholder(tf.float32, [batch_size_f, ZonalNum, n_input_f], name = \"X_f\")\n",
    "    _Y_f = tf.placeholder(tf.float32, [batch_size_f, ZonalNum, n_output_f], name = \"Y_f\")\n",
    "    _Dropout_f = tf.placeholder(tf.float32, name = \"Dropout_f\")\n",
    "    _X_h = tf.placeholder(tf.float32, [test_batch_size_h, n_input_h])\n",
    "    _Y_h = tf.placeholder(tf.float32, [test_batch_size_h, n_output_h])\n",
    "\n",
    "    # Create model\n",
    "    def MLP_f(x, _dropout, weights, biases):    \n",
    "        x = tf.reshape(x, [-1, n_input_f])\n",
    "        # Hidden layer with RELU activation\n",
    "        layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "        layer_1 = tf.nn.sigmoid(layer_1)\n",
    "        #layer_1 = tf.nn.dropout(layer_1,_dropout)\n",
    "\n",
    "        # Hidden layer with RELU activation\n",
    "        layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "        layer_2 = tf.nn.sigmoid(layer_2)\n",
    "        #layer_2 = tf.nn.dropout(layer_2,_dropout)\n",
    "\n",
    "        # Hidden layer with RELU activation\n",
    "        layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
    "        layer_3 = tf.nn.sigmoid(layer_3)\n",
    "        #layer_3 = tf.nn.dropout(layer_3,_dropout)\n",
    "\n",
    "        # Hidden layer with RELU activation\n",
    "        layer_4 = tf.add(tf.matmul(layer_3, weights['h4']), biases['b4'])\n",
    "        layer_4 = tf.nn.sigmoid(layer_4)\n",
    "\n",
    "        # Output layer with linear activation\n",
    "        result = tf.matmul(layer_4, weights['out']) + biases['out']\n",
    "        result = tf.nn.sigmoid(result)\n",
    "        #result = tf.nn.dropout(result,_dropout)\n",
    "        return result\n",
    "\n",
    "\n",
    "    def MLP_h(x, weights, biases):\n",
    "        # Hidden layer with RELU activation\n",
    "        layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "        layer_1 = tf.nn.sigmoid(layer_1)\n",
    "        #layer_1 = tf.nn.dropout(layer_1,_dropout)\n",
    "\n",
    "        # Hidden layer with RELU activation\n",
    "        layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "        layer_2 = tf.nn.sigmoid(layer_2)\n",
    "        #layer_2 = tf.nn.dropout(layer_2,_dropout)\n",
    "\n",
    "        # Hidden layer with RELU activation\n",
    "        layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
    "        layer_3 = tf.nn.sigmoid(layer_3)\n",
    "        #layer_3 = tf.nn.dropout(layer_3,_dropout)\n",
    "\n",
    "        # Output layer with linear activation\n",
    "        result = tf.matmul(layer_3, weights['out']) + biases['out']\n",
    "        result = tf.nn.sigmoid(result)\n",
    "        #result = tf.nn.dropout(result,_dropout)\n",
    "        return result\n",
    "\n",
    "    # MLP\n",
    "    weights_f = {\n",
    "        'h1': tf.Variable(tf.random_normal([n_input_f, n_hidden_f_1]), name = \"w_f_1\"),\n",
    "        'h2': tf.Variable(tf.random_normal([n_hidden_f_1, n_hidden_f_2]), name = \"w_f_2\"),\n",
    "        'h3': tf.Variable(tf.random_normal([n_hidden_f_2, n_hidden_f_3]), name = \"w_f_3\"),\n",
    "        'h4': tf.Variable(tf.random_normal([n_hidden_f_3, n_hidden_f_4]), name = \"w_f_4\"),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden_f_4, n_output_f]), name = \"w_o\")\n",
    "    }\n",
    "    biases_f = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden_f_1]), name = \"b_f_1\"),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden_f_2]), name = \"b_f_2\"),\n",
    "        'b3': tf.Variable(tf.random_normal([n_hidden_f_3]), name = \"b_f_3\"),\n",
    "        'b4': tf.Variable(tf.random_normal([n_hidden_f_4]), name = \"b_f_4\"),\n",
    "        'out': tf.Variable(tf.random_normal([n_output_f]), name = \"b_o\")\n",
    "    }\n",
    "    weights_h = {\n",
    "        'h1': tf.Variable(tf.random_normal([n_input_h, n_hidden_h_1])),\n",
    "        'h2': tf.Variable(tf.random_normal([n_hidden_h_1, n_hidden_h_2])),\n",
    "        'h3': tf.Variable(tf.random_normal([n_hidden_h_2, n_hidden_h_3])),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden_h_3, n_output_h]))\n",
    "    }\n",
    "    biases_h = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden_h_1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden_h_2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_hidden_h_3])),\n",
    "        'out': tf.Variable(tf.random_normal([n_output_h]))\n",
    "    }\n",
    "    p_f = MLP_f(_X_f, _Dropout_f, weights_f, biases_f)\n",
    "    p_h = MLP_h(_X_h,  weights_h, biases_h)\n",
    "    p_f = tf.reshape(p_f, [test_batch_size_h, s3_input])\n",
    "    p_h = tf.reshape(p_h, [test_batch_size_h, s3_input])\n",
    "    tao_s3 = tf.placeholder(tf.float32)\n",
    "    _I = tf.concat(1, [p_f,p_h])\n",
    "    _O = _Y_h\n",
    "    weights_s3 = {\n",
    "        'h1': tf.Variable(tf.random_normal([2*s3_input, n_hidden_s3_1])),\n",
    "        'h2': tf.Variable(tf.random_normal([n_hidden_s3_1, n_hidden_s3_2])),\n",
    "        #'h3': tf.Variable(tf.random_normal([n_hidden_f_2, n_hidden_f_3])),\n",
    "        #'h4': tf.Variable(tf.random_normal([n_hidden_f_3, n_hidden_f_4])),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden_s3_2, s3_output]))\n",
    "    }\n",
    "    biases_s3 = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden_s3_1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden_s3_2])),\n",
    "        #'b3': tf.Variable(tf.random_normal([n_hidden_f_3])),\n",
    "        #'b4': tf.Variable(tf.random_normal([n_hidden_f_4])),\n",
    "        'out': tf.Variable(tf.random_normal([s3_output]))\n",
    "    }\n",
    "    def MLP_s3(x, weights, biases):\n",
    "        # Hidden layer with RELU activation\n",
    "        layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "        layer_1 = tf.nn.sigmoid(layer_1)\n",
    "        #layer_1 = tf.nn.dropout(layer_1,_dropout)\n",
    "\n",
    "        # Hidden layer with RELU activation\n",
    "        layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "        layer_2 = tf.nn.sigmoid(layer_2)\n",
    "        #layer_2 = tf.nn.dropout(layer_2,_dropout)\n",
    "        # Output layer with linear activation\n",
    "        result = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "        result = tf.nn.sigmoid(result)\n",
    "        #result = tf.nn.dropout(result,_dropout)\n",
    "        return result\n",
    "\n",
    "    _P = MLP_s3(_I, weights_s3, biases_s3)\n",
    "\n",
    "    reshaped_outputs = tf.reshape(_P, [-1]) \n",
    "\n",
    "    reshaped_results = tf.reshape(_O,[-1])\n",
    "\n",
    "        #cost function\n",
    "    cost_s3 = 0.0\n",
    "    for i in range(s3_input*test_batch_size_h):\n",
    "        tmp = tf.cond(reshaped_results[i]-reshaped_outputs[i]>=0, lambda:(reshaped_results[i]-reshaped_outputs[i])*tao_s3,lambda:(reshaped_outputs[i]-reshaped_results[i])*(1.0-tao_s3))\n",
    "        cost_s3 = tf.add(tmp,cost_s3)\n",
    "    ccost_s3 = tf.reduce_mean(tf.pow(_O - _P,2)) \n",
    "\n",
    "    optimizer_s3 = tf.train.AdamOptimizer(learning_rate=0.001, beta1 = 0.8, beta2 = 0.7).minimize(ccost_s3)\n",
    "    optimizer_s4 = tf.train.AdamOptimizer(learning_rate=0.0001, beta1 = 0.8, beta2 = 0.7).minimize(cost_s3)\n",
    "    #n_hidden_s3_3 = 30\n",
    "    #n_hidden_s3_4 = 30\n",
    "\n",
    "\n",
    "def maxe(predictions, targets):\n",
    "    return np.max(abs(predictions-targets))\n",
    "\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "\n",
    "def mape(predictions, targets):\n",
    "    return np.mean(abs(predictions-targets)/targets)\n",
    "def variable_summaries(var, name):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor.\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.scalar_summary('mean/' + name, mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))\n",
    "        tf.scalar_summary('sttdev/' + name, stddev)\n",
    "        tf.scalar_summary('max/' + name, tf.reduce_max(var))\n",
    "        tf.scalar_summary('min/' + name, tf.reduce_min(var))\n",
    "        tf.histogram_summary(name, var)\n",
    "\n",
    "def test_data_gen():\n",
    "    X_h = np.zeros((test_batch_size_h, n_input_h))\n",
    "    Y_h = np.zeros((test_batch_size_h, n_output_h))\n",
    "    X_f = np.zeros((test_batch_size_f,ZonalNum,n_input_f))\n",
    "    Y_f = np.zeros((test_batch_size_f,ZonalNum,n_output_f))\n",
    "    count_h = 0\n",
    "    count_f = 0\n",
    "    for i in test_id_f:\n",
    "        Y_f[count_f] = db_f[i,:,:1]\n",
    "        X_f[count_f] = db_f[i,:,1:]\n",
    "        count_f = count_f + 1\n",
    "    for i in test_id_h:\n",
    "        Y_h[count_h,:] = (db_h[i,:,0])\n",
    "        X_h[count_h,:] = (db_h[i-input_seq_size_h/output_seq_size_h-gap_h:i-gap_h,:,0]).reshape(n_input_h)\n",
    "        count_h = count_h + 1\n",
    "    X_h = X_h.astype(np.float32)\n",
    "    Y_h = Y_h.astype(np.float32)\n",
    "    X_f = X_f.astype(np.float32)\n",
    "    Y_f = Y_f.astype(np.float32)\n",
    "    return (X_h,Y_h,X_f,Y_f)\n",
    "\n",
    "def train_data_gen():\n",
    "    X_h = np.zeros((train_batch_size_h, n_input_h))\n",
    "    Y_h = np.zeros((train_batch_size_h, n_output_h))\n",
    "    X_f = np.zeros((train_batch_size_f,ZonalNum,n_input_f))\n",
    "    Y_f = np.zeros((train_batch_size_f,ZonalNum,n_output_f))\n",
    "    count_h = 0\n",
    "    count_f = 0\n",
    "    rang = range(0,train_id_h.shape[0])\n",
    "    train_rd = rd.sample(rang,train_batch_size_h)\n",
    "    train_rd = np.sort(train_rd)\n",
    "    for i in train_rd:\n",
    "        k = train_id_h[i]\n",
    "        Y_h[count_h] = db_h[k,:,0]\n",
    "        X_h[count_h] = (db_h[k-input_seq_size_h/output_seq_size_h-gap_h:k-gap_h,:,0]).reshape(n_input_h)\n",
    "        for j in range(24):\n",
    "            Y_f[count_f] = db_f[k*24+j,:,:1]\n",
    "            X_f[count_f] = db_f[k*24+j,:,1:]\n",
    "            count_f = count_f + 1\n",
    "        count_h = count_h + 1\n",
    "    X_h = X_h.astype(np.float32)\n",
    "    Y_h = Y_h.astype(np.float32)\n",
    "    X_f = X_f.astype(np.float32)\n",
    "    Y_f = Y_f.astype(np.float32)\n",
    "    return (X_h,Y_h,X_f,Y_f)\n",
    "\n",
    "def valid_data_gen():\n",
    "    X_h = np.zeros((valid_batch_size_h, n_input_h))\n",
    "    Y_h = np.zeros((valid_batch_size_h, n_output_h))\n",
    "    X_f = np.zeros((valid_batch_size_f,ZonalNum,n_input_f))\n",
    "    Y_f = np.zeros((valid_batch_size_f,ZonalNum,n_output_f))\n",
    "    count_h = 0\n",
    "    count_f = 0\n",
    "    rang = range(0,valid_id_h.shape[0])\n",
    "    valid_rd = rd.sample(rang,train_batch_size_h)\n",
    "    valid_rd = np.sort(valid_rd)\n",
    "    for i in train_rd:\n",
    "        k = valid_id_h[i]\n",
    "        Y_h[count_h] = db_h[k,:,0]\n",
    "        X_h[count_h] = (db_h[k-input_seq_size_h/output_seq_size_h-gap_h:k-gap_h,:,0]).reshape(n_input_h)\n",
    "        for j in range(24):\n",
    "            Y_f[count_f] = db_f[k*24+j,:,:1]\n",
    "            X_f[count_f] = db_f[k*24+j,:,1:]\n",
    "            count_f = count_f + 1\n",
    "        count_h = count_h + 1\n",
    "    X_h = X_h.astype(np.float32)\n",
    "    Y_h = Y_h.astype(np.float32)\n",
    "    X_f = X_f.astype(np.float32)\n",
    "    Y_f = Y_f.astype(np.float32)\n",
    "    return (X_h,Y_h,X_f,Y_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Minibatch Loss ---- Pinball = 4.32387   ****   RMSE = 0.0753565\n",
      "Iter 100, Minibatch Loss ---- Pinball = 3.6094   ****   RMSE = 0.0673168\n",
      "Iter 200, Minibatch Loss ---- Pinball = 3.75276   ****   RMSE = 0.0521283\n",
      "Iter 300, Minibatch Loss ---- Pinball = 4.08696   ****   RMSE = 0.0785011\n",
      "Iter 400, Minibatch Loss ---- Pinball = 3.56601   ****   RMSE = 0.0624135\n",
      "Iter 500, Minibatch Loss ---- Pinball = 3.7429   ****   RMSE = 0.0592182\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-71f59c58daef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mx_h\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_h\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_f\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_f\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data_gen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0my_s3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_h\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moptimizer_s4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0m_X_f\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Y_f\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_X_h\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Y_h\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtao_s3\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtmp_tao\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mx_h\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_h\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_f\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_f\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data_gen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/codefisheng/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    708\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 710\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    711\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/codefisheng/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    906\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 908\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    909\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/codefisheng/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    956\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    957\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 958\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m    959\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/home/codefisheng/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    963\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 965\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/codefisheng/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m    945\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m    946\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m    948\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "taolist = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "time1 = time.time()\n",
    "for t in xrange(0,9):\n",
    "    tmp_tao = taolist[t]\n",
    "    saver = tf.train.Saver({\"wf1\":weights_f['h1'],\"wf2\":weights_f['h2'],\"wf3\":weights_f['h3'],\"wf4\":weights_f['h4'],\"wfo\":weights_f['out'],\"bf1\":biases_f['b1'],\"bf2\":biases_f['b2'],\"bf3\":biases_f['b3'],\"bf4\":biases_f['b4'],\"bfo\":biases_f['out'],'wh1':weights_h['h1'],'wh2':weights_h['h2'],'wh3':weights_h['h3'],'who':weights_h['out'],'bh1':biases_h['b1'],'bh2':biases_h['b2'],'bh3':biases_h['b3'],'bho':biases_h['out'],\"wf1\":weights_f['h1'],\"wf2\":weights_f['h2'],\"wf3\":weights_f['h3'],\"wf4\":weights_f['h4'],\"wfo\":weights_f['out'],\"bf1\":biases_f['b1'],\"bf2\":biases_f['b2'],\"bf3\":biases_f['b3'],\"bf4\":biases_f['b4'],\"bfo\":biases_f['out'],'wh1':weights_h['h1'],'wh2':weights_h['h2'],'wh3':weights_h['h3'],'who':weights_h['out'],'bh1':biases_h['b1'],'bh2':biases_h['b2'],'bh3':biases_h['b3'],'bho':biases_h['out'],\"ws1\":weights_s3['h1'],\"ws2\":weights_s3['h2'],\"wso\":weights_s3['out'],'bs1':biases_s3['b1'],'bs2':biases_s3['b2'],'bso':biases_s3['out']})\n",
    "    sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "    sess.run(init)\n",
    "    saver.restore(sess,\"C-models/model-ME-rmse-pinball-\"+str(tmp_tao)+ \"-checkpoint-4g-hs.ckpt\")\n",
    "    costs_train = []\n",
    "    costs_test = []\n",
    "    predlist = []\n",
    "    for i in range(n_epochs):\n",
    "        # training process\n",
    "        x_h,y_h,x_f,y_f = train_data_gen()\n",
    "        y_s3 = y_h\n",
    "        _ = sess.run([optimizer_s4], feed_dict = {_X_f: x_f, _Y_f: y_f, _X_h: x_h, _Y_h: y_h, tao_s3: tmp_tao})\n",
    "        if i % 100 == 0:\n",
    "            x_h,y_h,x_f,y_f = train_data_gen()\n",
    "            P,  err_t, err_c = sess.run([_P, cost_s3,ccost_s3], feed_dict = {_X_f: x_f, _Y_f: y_f, _X_h: x_h, _Y_h: y_h, tao_s3: tmp_tao})\n",
    "            print \"Iter \" + str(i) + \", Minibatch Loss ---- Pinball = \" + str(err_t) + \"   ****   RMSE = \" + str(np.sqrt(err_c))\n",
    "            P = np.reshape(np.array(P),[-1])\n",
    "            costs_test.append(err_t)\n",
    "            predlist.append(P)\n",
    "        if i == n_epochs-1:\n",
    "            x_h,y_h,x_f,y_f = train_data_gen()\n",
    "            P,  err_t, err_c = sess.run([_P, cost_s3,ccost_s3], feed_dict = {_X_f: x_f, _Y_f: y_f, _X_h: x_h, _Y_h: y_h, tao_s3: tmp_tao})\n",
    "            print \"Iter \" + str(i) + \", Minibatch Loss ---- Pinball = \" + str(err_t) + \"   ****   RMSE = \" + str(np.sqrt(err_c))\n",
    "            P = np.reshape(np.array(P),[-1])\n",
    "            costs_test.append(err_t)\n",
    "            predlist.append(P)\n",
    "    saver.save(sess,\"C-models/model-ME-rmse-pinball-\"+str(tmp_tao)+ \"-checkpoint-h-hs.ckpt\")\n",
    "    print \"C-models/model-ME-rmse-pinball-\"+str(tmp_tao)+ \"-checkpoint-h-hs.ckpt\"\n",
    "    \n",
    "    costs_train = []\n",
    "    costs_test = []\n",
    "    predlist = []\n",
    "    for i in range(n_epochs):\n",
    "        # training process\n",
    "        x_h,y_h,x_f,y_f = train_data_gen()\n",
    "        y_s3 = y_h\n",
    "        _ = sess.run([optimizer_s4], feed_dict = {_X_f: x_f, _Y_f: y_f, _X_h: x_h, _Y_h: y_h, tao_s3: tmp_tao})\n",
    "        if i % 100 == 0:\n",
    "            x_h,y_h,x_f,y_f = train_data_gen()\n",
    "            P,  err_t, err_c = sess.run([_P, cost_s3,ccost_s3], feed_dict = {_X_f: x_f, _Y_f: y_f, _X_h: x_h, _Y_h: y_h, tao_s3: tmp_tao})\n",
    "            print \"Iter \" + str(i) + \", Minibatch Loss ---- Pinball = \" + str(err_t) + \"   ****   RMSE = \" + str(np.sqrt(err_c))\n",
    "            P = np.reshape(np.array(P),[-1])\n",
    "            costs_test.append(err_t)\n",
    "            predlist.append(P)\n",
    "        if i == n_epochs-1:\n",
    "            x_h,y_h,x_f,y_f = train_data_gen()\n",
    "            P,  err_t, err_c = sess.run([_P, cost_s3,ccost_s3], feed_dict = {_X_f: x_f, _Y_f: y_f, _X_h: x_h, _Y_h: y_h, tao_s3: tmp_tao})\n",
    "            print \"Iter \" + str(i) + \", Minibatch Loss ---- Pinball = \" + str(err_t) + \"   ****   RMSE = \" + str(np.sqrt(err_c))\n",
    "            P = np.reshape(np.array(P),[-1])\n",
    "            costs_test.append(err_t)\n",
    "            predlist.append(P)\n",
    "    saver.save(sess,\"C-models/model-ME-rmse-pinball-\"+str(tmp_tao)+ \"-checkpoint-i-hs.ckpt\")\n",
    "    print \"C-models/model-ME-rmse-pinball-\"+str(tmp_tao)+ \"-checkpoint-i-hs.ckpt\"\n",
    "    \n",
    "    costs_train = []\n",
    "    costs_test = []\n",
    "    predlist = []\n",
    "    for i in range(n_epochs):\n",
    "        # training process\n",
    "        x_h,y_h,x_f,y_f = train_data_gen()\n",
    "        y_s3 = y_h\n",
    "        _ = sess.run([optimizer_s4], feed_dict = {_X_f: x_f, _Y_f: y_f, _X_h: x_h, _Y_h: y_h, tao_s3: tmp_tao})\n",
    "        if i % 100 == 0:\n",
    "            x_h,y_h,x_f,y_f = train_data_gen()\n",
    "            P,  err_t, err_c = sess.run([_P, cost_s3,ccost_s3], feed_dict = {_X_f: x_f, _Y_f: y_f, _X_h: x_h, _Y_h: y_h, tao_s3: tmp_tao})\n",
    "            print \"Iter \" + str(i) + \", Minibatch Loss ---- Pinball = \" + str(err_t) + \"   ****   RMSE = \" + str(np.sqrt(err_c))\n",
    "            P = np.reshape(np.array(P),[-1])\n",
    "            costs_test.append(err_t)\n",
    "            predlist.append(P)\n",
    "        if i == n_epochs-1:\n",
    "            x_h,y_h,x_f,y_f = train_data_gen()\n",
    "            P,  err_t, err_c = sess.run([_P, cost_s3,ccost_s3], feed_dict = {_X_f: x_f, _Y_f: y_f, _X_h: x_h, _Y_h: y_h, tao_s3: tmp_tao})\n",
    "            print \"Iter \" + str(i) + \", Minibatch Loss ---- Pinball = \" + str(err_t) + \"   ****   RMSE = \" + str(np.sqrt(err_c))\n",
    "            P = np.reshape(np.array(P),[-1])\n",
    "            costs_test.append(err_t)\n",
    "            predlist.append(P)\n",
    "    saver.save(sess,\"C-models/model-ME-rmse-pinball-\"+str(tmp_tao)+ \"-checkpoint-j-hs.ckpt\")\n",
    "    print \"C-models/model-ME-rmse-pinball-\"+str(tmp_tao)+ \"-checkpoint-j-hs.ckpt\"\n",
    "    \n",
    "    costs_train = []\n",
    "    costs_test = []\n",
    "    predlist = []\n",
    "    for i in range(n_epochs):\n",
    "        # training process\n",
    "        x_h,y_h,x_f,y_f = train_data_gen()\n",
    "        y_s3 = y_h\n",
    "        _ = sess.run([optimizer_s4], feed_dict = {_X_f: x_f, _Y_f: y_f, _X_h: x_h, _Y_h: y_h, tao_s3: tmp_tao})\n",
    "        if i % 100 == 0:\n",
    "            x_h,y_h,x_f,y_f = train_data_gen()\n",
    "            P,  err_t, err_c = sess.run([_P, cost_s3,ccost_s3], feed_dict = {_X_f: x_f, _Y_f: y_f, _X_h: x_h, _Y_h: y_h, tao_s3: tmp_tao})\n",
    "            print \"Iter \" + str(i) + \", Minibatch Loss ---- Pinball = \" + str(err_t) + \"   ****   RMSE = \" + str(np.sqrt(err_c))\n",
    "            P = np.reshape(np.array(P),[-1])\n",
    "            costs_test.append(err_t)\n",
    "            predlist.append(P)\n",
    "        if i == n_epochs-1:\n",
    "            x_h,y_h,x_f,y_f = train_data_gen()\n",
    "            P,  err_t, err_c = sess.run([_P, cost_s3,ccost_s3], feed_dict = {_X_f: x_f, _Y_f: y_f, _X_h: x_h, _Y_h: y_h, tao_s3: tmp_tao})\n",
    "            print \"Iter \" + str(i) + \", Minibatch Loss ---- Pinball = \" + str(err_t) + \"   ****   RMSE = \" + str(np.sqrt(err_c))\n",
    "            P = np.reshape(np.array(P),[-1])\n",
    "            costs_test.append(err_t)\n",
    "            predlist.append(P)\n",
    "    saver.save(sess,\"C-models/model-ME-rmse-pinball-\"+str(tmp_tao)+ \"-checkpoint-k-hs.ckpt\")\n",
    "    print \"C-models/model-ME-rmse-pinball-\"+str(tmp_tao)+ \"-checkpoint-k-hs.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rmseList = []\n",
    "mapeList = []\n",
    "x_h,y_h,x_f,y_f = test_data_gen()\n",
    "acload = np.reshape(np.array(y_h), [-1])\n",
    "for prload in predlist:\n",
    "    prload = np.reshape(np.array(prload), [-1])\n",
    "    rmseList.append(rmse(prload,acload))\n",
    "    mapeList.append(mape(prload,acload))\n",
    "print \"Final model RMSE = \" + str(np.mean(rmseList[-11:-1]))\n",
    "print rmseList[-11:]\n",
    "print \"Final model MAPE = \" + str(np.mean(mapeList[-11:-1]))\n",
    "print mapeList[-11:]\n",
    "print time.time()-time1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "l = tf.placeholder(tf.float32, [24*31])\n",
    "r = tf.placeholder(tf.float32, [24*31])\n",
    "cost_s3 = tf.Variable(initial_value = 0.0)\n",
    "    #cost function\n",
    "for i in range(24*31):\n",
    "    tmp = tf.cond(l[i]-r[i]>=0, lambda:(l[i]-r[i])*0.1,lambda:(r[i]-l[i])*(1.0-0.1))\n",
    "    cost_s3 = tf.add(tmp,cost_s3)\n",
    "cost_s3 = cost_s3/31\n",
    "    \n",
    "init = tf.initialize_all_variables()\n",
    "sess1 = tf.Session()\n",
    "sess1.run(init)\n",
    "print sess1.run(cost_s3, feed_dict = {l:P,r:y_f})\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "ac = np.array(acload)\n",
    "prload = predlist[-1]\n",
    "pr = np.array(np.reshape(np.array(prload), [-1]))\n",
    "t = np.array(range(ac.shape[0]))\n",
    "\n",
    "label_f0 = r\"real load\"\n",
    "label_f1 = r\"predicted load\"\n",
    "\n",
    "p1 = pl.subplot()\n",
    "p1.plot(t,ac,\"g\",label=label_f0,linewidth=2)\n",
    "p1.plot(t,pr,\"r:\",label=label_f1,linewidth=2)\n",
    "\n",
    "#p1.axis([0,700,10000,25000])\n",
    "\n",
    "p1.set_ylabel(\"Demand (kWh)\",fontsize=14)\n",
    "p1.set_xlabel(\"Time Point\",fontsize=14)\n",
    "#p1.set_title(\"A simple example\",fontsize=18)\n",
    "p1.grid(True)\n",
    "p1.legend()\n",
    "\n",
    "pl.show()\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
